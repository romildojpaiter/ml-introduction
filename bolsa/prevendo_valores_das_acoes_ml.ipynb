{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de deep learning baseado na memória de curto prazo (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.2\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas_datareader\n",
      "  Using cached pandas_datareader-0.10.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting lxml (from pandas_datareader)\n",
      "  Downloading lxml-5.2.2-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting pandas>=0.23 (from pandas_datareader)\n",
      "  Downloading pandas-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting requests>=2.19.0 (from pandas_datareader)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/romildopaiter/.pyenv/versions/3.9.2/lib/python3.9/site-packages (from pandas>=0.23->pandas_datareader) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/romildopaiter/.pyenv/versions/3.9.2/lib/python3.9/site-packages (from pandas>=0.23->pandas_datareader) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=0.23->pandas_datareader)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=0.23->pandas_datareader)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.19.0->pandas_datareader)\n",
      "  Downloading charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.19.0->pandas_datareader)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->pandas_datareader)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.19.0->pandas_datareader)\n",
      "  Using cached certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/romildopaiter/.pyenv/versions/3.9.2/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=0.23->pandas_datareader) (1.16.0)\n",
      "Using cached pandas_datareader-0.10.0-py3-none-any.whl (109 kB)\n",
      "Downloading pandas-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading lxml-5.2.2-cp39-cp39-manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Downloading charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.3/142.3 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: pytz, urllib3, tzdata, lxml, idna, charset-normalizer, certifi, requests, pandas, pandas_datareader\n",
      "Successfully installed certifi-2024.7.4 charset-normalizer-3.3.2 idna-3.7 lxml-5.2.2 pandas-2.2.2 pandas_datareader-0.10.0 pytz-2024.1 requests-2.32.3 tzdata-2024.1 urllib3-2.2.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas_datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler \n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, LSTM\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pandas_datareader import data as pdr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acao = \"MGLU3.SA\"\n",
    "\n",
    "inicio = \"2014-12-31\"\n",
    "final = \"2022-09-15\"\n",
    "\n",
    "dados_acao = pdr.get_data_yahoo(acao, inicio, final)\n",
    "\n",
    "dados_acao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nao pode ser ajustados\n",
    "\n",
    "cotacao = dados_acao['Close'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "cotacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamanho_dados_treinamento = int(len(cotacao) * 0.8)\n",
    "\n",
    "tamanho_dados_treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#escalar os dados entre 0 e 1, para deixar mais fácil o processamento\n",
    "#dados em escala pré definidas são mais fáceis de lidar. \n",
    "\n",
    "escalador = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "dados_entre_0_e_1_treinamento = escalador.fit_transform(cotacao[0: tamanho_dados_treinamento, :])\n",
    "\n",
    "dados_entre_0_e_1_teste = escalador.transform(cotacao[tamanho_dados_treinamento: , :])\n",
    "\n",
    "dados_entre_0_e_1 = list(dados_entre_0_e_1_treinamento.reshape(\n",
    "    len(dados_entre_0_e_1_treinamento))) + list(dados_entre_0_e_1_teste.reshape(len(dados_entre_0_e_1_teste)))\n",
    "                                                \n",
    "\n",
    "dados_entre_0_e_1 = np.array(dados_entre_0_e_1).reshape(len(dados_entre_0_e_1), 1)\n",
    "\n",
    "dados_entre_0_e_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_para_treinamento = dados_entre_0_e_1[0: tamanho_dados_treinamento, :]\n",
    "\n",
    "#dados que serão usados para gerar o resultado\n",
    "treinamento_x = []\n",
    "#cotação que aconteceu de fato\n",
    "treinamento_y = []\n",
    "\n",
    "\n",
    "for i in range(60, len(dados_para_treinamento)):\n",
    "\n",
    "    #60 ultimos dias\n",
    "    treinamento_x.append(dados_para_treinamento[i - 60: i, 0])\n",
    "    #cotacao\n",
    "    treinamento_y.append(dados_para_treinamento[i, 0])\n",
    "\n",
    "    if i <= 61:\n",
    "\n",
    "        print(treinamento_x)\n",
    "        print(treinamento_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformando as listas em arrays e dando reshape 3d \n",
    "\n",
    "treinamento_x, treinamento_y = np.array(treinamento_x), np.array(treinamento_y)\n",
    "\n",
    "print(treinamento_x)\n",
    "\n",
    "treinamento_x = treinamento_x.reshape(treinamento_x.shape[0], treinamento_x.shape[1], 1)\n",
    "\n",
    "print(treinamento_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construindo o modelo\n",
    "\n",
    "modelo = Sequential()\n",
    "\n",
    "#vamos criar um modelo com 50 neurônios\n",
    "#return sequences = True pois vamos usar outro LSTM depois.\n",
    "#definir o shape, que no caso são 60 informações para gerar uma.\n",
    "#Adicionar mais neurônios com o dense, 25 e 1\n",
    "#Não se apegue a isso agora, é apenas um arquitetura de deep learning.\n",
    "\n",
    "modelo.add(LSTM(50, return_sequences=True, input_shape = (treinamento_x.shape[1], 1)))\n",
    "modelo.add(LSTM(50, return_sequences=False))\n",
    "modelo.add(Dense(25))\n",
    "modelo.add(Dense(1))\n",
    "\n",
    "treinamento_x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copilando o modelo\n",
    "\n",
    "# a função de loss é a forma de medir o erro do modelo, que nesse caso\n",
    "#é o classico erro médio quadrático da que é usado em regressão linear\n",
    "#otimizador e medida de erro\n",
    "\n",
    "modelo.compile(optimizer=\"adam\", loss=\"mean_squared_error\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agora com o modelo copilado e os dados, podemos treinar o modelo\n",
    "#batch size é depois de quantas em quantas amostras o modelo irá otimizar os parâmetros.\n",
    "#epochs é quantas vezes o algoritmo irá rodar os dados treinamento, aprendendo. \n",
    "\n",
    "modelo.fit(treinamento_x, treinamento_y, batch_size=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criar dados de teste\n",
    "\n",
    "dados_teste = dados_entre_0_e_1[tamanho_dados_treinamento - 60:, :]\n",
    "\n",
    "teste_x = []\n",
    "teste_y = cotacao[tamanho_dados_treinamento: , :] \n",
    "\n",
    "for i in range(60, len(dados_teste)):\n",
    "    teste_x.append(dados_teste[i - 60: i, 0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape\n",
    "teste_x = np.array(teste_x)\n",
    "teste_x = teste_x.reshape(teste_x.shape[0], teste_x.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pegando predições do modelo\n",
    "\n",
    "predicoes = modelo.predict(teste_x)\n",
    "\n",
    "#tirando a escala dos dados\n",
    "\n",
    "predicoes = escalador.inverse_transform(predicoes)\n",
    "\n",
    "predicoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pegando o erro médio quadrático (RMSE)\n",
    "\n",
    "rmse = np.sqrt(np.mean(predicoes - teste_y) ** 2)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando o grafico do modelo\n",
    "\n",
    "\n",
    "treinamento = dados_acao.iloc[:tamanho_dados_treinamento, :]\n",
    "df_teste = pd.DataFrame({\"Close\": dados_acao['Close'].iloc[tamanho_dados_treinamento:],\n",
    "                        \"predicoes\": predicoes.reshape(len(predicoes))})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(16, 8))\n",
    "plt.title('Modelo')\n",
    "plt.xlabel('Data', fontsize = 18)\n",
    "plt.ylabel(\"Preço de fechamento\", fontsize = 18)\n",
    "plt.plot(treinamento['Close'])\n",
    "plt.plot(df_teste[['Close', 'predicoes']])\n",
    "plt.legend(['Treinamento', 'Real', 'Predições'], loc=2, prop={'size': 16})\n",
    "plt.show()\n",
    "\n",
    "#essa queda pegou o modelo despevinido, pois MGLU só subia até então praticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste.sort_index()\n",
    "\n",
    "df_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#o preço é legal, mas o importante é acertar pra qual mercado o lado vai. Sera q isso foi feito?\n",
    "\n",
    "#calcular media de acertos e expectativa de lucro\n",
    "\n",
    "df_teste['variacao_percentual_acao'] = df_teste['Close'].pct_change()\n",
    "df_teste['variacao_percentual_modelo'] = df_teste['predicoes'].pct_change()\n",
    "\n",
    "df_teste = df_teste.dropna()\n",
    "\n",
    "df_teste['var_acao_maior_menor_que_zero'] = np.where(df_teste['variacao_percentual_acao'] > 0, \n",
    "                                                      True, False)\n",
    "df_teste['var_modelo_maior_menor_que_zero'] = np.where(df_teste['variacao_percentual_modelo'] > 0, \n",
    "                                                      True, False)\n",
    "\n",
    "df_teste['acertou_o_lado'] = np.where(df_teste['var_acao_maior_menor_que_zero'] == df_teste['var_modelo_maior_menor_que_zero']\n",
    "                                      , True, False)\n",
    "\n",
    "df_teste['variacao_percentual_acao_abs'] = df_teste['variacao_percentual_acao'].abs()\n",
    "\n",
    "df_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acertou_lado = df_teste['acertou_o_lado'].sum()/len(df_teste['acertou_o_lado'])\n",
    "errou_lado = 1 - acertou_lado\n",
    "\n",
    "media_lucro = df_teste.groupby('acertou_o_lado')['variacao_percentual_acao_abs'].mean()\n",
    "\n",
    "exp_mat_lucro = acertou_lado * media_lucro[1] - media_lucro[0] * errou_lado\n",
    "\n",
    "ganho_sobre_perda = media_lucro[1]/media_lucro[0]\n",
    "\n",
    "print(media_lucro)\n",
    "print(ganho_sobre_perda)\n",
    "print(acertou_lado)\n",
    "print(exp_mat_lucro * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando um código que você passa 60 dias e ele devolve a cotação\n",
    "#resumindo: vamos descobrir o preço da petrobras de hoje/amanha com esse modelo\n",
    "\n",
    "data_hoje = datetime.now()\n",
    "\n",
    "#se quiser escolher um dia, basta fazer assim\n",
    "\n",
    "#data_hoje = datetime.now() - timedelta(days = 1)\n",
    "\n",
    "if data_hoje.hour > 18:\n",
    "    \n",
    "    final = data_hoje\n",
    "    inicial = datetime.now() - timedelta(days = 252)\n",
    "    \n",
    "else:\n",
    "    final = data_hoje - timedelta(days = 1)\n",
    "    inicial = datetime.now() - timedelta(days = 252)\n",
    "    \n",
    "#nao vai botar outra ação aqui hein kkkkkkkk\n",
    "cotacoes = pdr.get_data_yahoo(acao, inicial, final) \n",
    "ultimos_60_dias = cotacoes['Close'].iloc[-60:].values.reshape(-1, 1)\n",
    "\n",
    "ultimos_60_dias_escalado = escalador.transform(ultimos_60_dias)\n",
    "\n",
    "teste_x = []\n",
    "teste_x.append(ultimos_60_dias_escalado)\n",
    "teste_x = np.array(teste_x)\n",
    "teste_x = teste_x.reshape(teste_x.shape[0], teste_x.shape[1], 1)\n",
    "\n",
    "previsao_de_preco = modelo.predict(teste_x)\n",
    "previsao_de_preco = escalador.inverse_transform(previsao_de_preco)\n",
    "\n",
    "print(previsao_de_preco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sugestões:\n",
    "<br>\n",
    "\n",
    "- Melhorar as estatísticas de avaliação (dias seguidos ganhando, max DD, etc)\n",
    "<br>\n",
    "\n",
    "- Rodar pra todas as ações do ibovespa e criar uma expectativa matemática da expectativa matemática. Isso vai deixar o resultado final ainda mais robusto, definando a aloção do $ basedo na liquidez\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "60c216e46b94f58a5fde426b81c8807da9e6291eb1596ef61e4d2d1672a8d8af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
